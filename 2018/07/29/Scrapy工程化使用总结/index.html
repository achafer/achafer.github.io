<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Scrapy工程化使用总结 · 远山淡影</title><meta name="description" content="Scrapy工程化使用总结 - 阿扯"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/gandalfr.css"><link rel="stylesheet" href="https://highlightjs.org/static/demo/styles/solarized-light.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.ache.fun/atom.xml" title="远山淡影"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/links/" target="_self" class="nav-list-link">LINKS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Scrapy工程化使用总结</h1><div class="tags"><a href="/tags/Python/" class="tag-title">#Python</a><a href="/tags/爬虫/" class="tag-title">#爬虫</a><a href="/tags/Scrapy/" class="tag-title">#Scrapy</a></div><div class="post-info">2018年7月29日</div><div class="post-content"><p>最近在做公司的一个项目涉及爬虫，语言习惯一开始想用golang解决，使用goquery，后发现，golang和goquery在处理多个站点后数据整合时，还需要做很多事情，在有限的时间内要快速完成，只能转移到生态更为成熟的<code>Scrapy</code>, 花了两周时间做了个多个站点的数据抓取整合的项目，其中有很多值得记录的地方，一并写下</p>
<h2 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h2><ol>
<li>使用<code>scrapy shell</code><a href="https://doc.scrapy.org/en/latest/topics/shell.html" target="_blank" rel="noopener">文档</a>设置User-Agent访问抓取目标页面</li>
<li>在shell中使用view(response)检查抓取的页面是否正确</li>
<li>使用<a href="https://doc.scrapy.org/en/latest/topics/selectors.html" target="_blank" rel="noopener">Selector</a>提取需要的数据</li>
<li>把提取规则复制到Spider中</li>
</ol>
<h2 id="运行多个Spider"><a href="#运行多个Spider" class="headerlink" title="运行多个Spider"></a>运行多个Spider</h2><p>在scrapy中，是使用命令行工具进行创建和控制项目，比如<code>scrapy startproject myproject</code>创建一个项目，<code>scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</code>创建一个spider，更多的命令参考<a href="https://doc.scrapy.org/en/latest/topics/commands.html#controlling-projects" target="_blank" rel="noopener">官方文档</a>，运行爬虫时使用<code>scrapy crawl</code>只能运行一个spider而且是运行完后就停止，可项目需求是定时运行多个spider，显然一般的命令行不能满足要求，在文档的<code>SOLVING SPECIFIC PROBLEMS</code>部分的<a href="https://doc.scrapy.org/en/latest/topics/practices.html" target="_blank" rel="noopener">Common Practices</a>发现可以使用脚本运行多个spider，所在此文件控制spider的运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerRunner</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.log <span class="keyword">import</span> configure_logging</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider1</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your first spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider2</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your second spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">configure_logging()</span><br><span class="line">runner = CrawlerRunner()</span><br><span class="line">runner.crawl(MySpider1)</span><br><span class="line">runner.crawl(MySpider2)</span><br><span class="line">d = runner.join()</span><br><span class="line">d.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br></pre></td></tr></table></figure>
<h2 id="循环运行Spider"><a href="#循环运行Spider" class="headerlink" title="循环运行Spider"></a>循环运行Spider</h2><p>完成运行多个Spider后，还需要定时的循环运行，定时可以利用第三方库，但循环运行spider的时候报<code>twisted.internet.error.ReactorNotRestartable</code>错误，面向stackoverflow编程后发现在同一个进程中不能重新启动reactor，最后在多个进程中使用，还没弄懂为什么，后面需要补一下twisted</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> scrapy.crawler <span class="keyword">as</span> crawler</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"></span><br><span class="line"><span class="comment"># your spider</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/tag/humor/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            print(quote.css(<span class="string">'span.text::text'</span>).extract_first())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the wrapper to make it run more times</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_spider</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(q)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            runner = crawler.CrawlerRunner()</span><br><span class="line">            deferred = runner.crawl(QuotesSpider)</span><br><span class="line">            deferred.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br><span class="line">            reactor.run()</span><br><span class="line">            q.put(<span class="keyword">None</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            q.put(e)</span><br><span class="line"></span><br><span class="line">    q = Queue()</span><br><span class="line">    p = Process(target=f, args=(q,))</span><br><span class="line">    p.start()</span><br><span class="line">    result = q.get()</span><br><span class="line">    p.join()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> result <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'first run:'</span>)</span><br><span class="line">run_spider()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nsecond run:'</span>)</span><br><span class="line">run_spider()</span><br></pre></td></tr></table></figure>
<h2 id="多个Spider指定对应的Item-Pipeline"><a href="#多个Spider指定对应的Item-Pipeline" class="headerlink" title="多个Spider指定对应的Item Pipeline"></a>多个Spider指定对应的Item Pipeline</h2><p>scrapy官方给的栗子定义Item Pipeline都是放在settings.py中，属于全局定义，运行每个spider都会生效。当需要给某个spider单独指定某个Item Pipelines而其他的spiser都不需要时，比如某个spider的自己清洗pipeline，只需在spider类中使用custom_setting属性，比如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'foo'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.foo.com'</span>]</span><br><span class="line">    download_delay = <span class="number">5</span></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">'ITEM_PIPELINES'</span>: &#123;</span><br><span class="line">            <span class="string">'example.pipelines.FooCleansingPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">'DOWNLOADER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'example.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>多种配置方式的优先级为命令行&gt;spider中的custom_settings&gt;项目settings&gt;各个命令的默认值&gt;全局默认值</p>
<h2 id="一个item-pipeline处理多个Item"><a href="#一个item-pipeline处理多个Item" class="headerlink" title="一个item pipeline处理多个Item"></a>一个item pipeline处理多个Item</h2><p>Spider生成多个Item时，在Spider中使用<code>yield item</code>都会传递到Item Pipeline，可需要分别对每个Item处理时，有个不优雅的解决办法，使用<code>isinstance</code>来判断进行处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooCleansingPipeline</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item, Item1):</span><br><span class="line">            ...</span><br><span class="line">        <span class="keyword">elif</span> isinstance(item, Item2):</span><br><span class="line">			...</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<h2 id="设置中的数值"><a href="#设置中的数值" class="headerlink" title="设置中的数值"></a>设置中的数值</h2><p>0-1000，从小到大依次执行</p>
<h2 id="一个Item在多个parse函数中传递"><a href="#一个Item在多个parse函数中传递" class="headerlink" title="一个Item在多个parse函数中传递"></a>一个Item在多个parse函数中传递</h2><p>有时候一个Item可能要在多个页面才能抓取完整，这时候就需要把Item传递到多个页面的解析回调函数中，在yield下一次Request时，可以在meta字段中传递Item，代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">      item = Item()</span><br><span class="line">      	...</span><br><span class="line"></span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">    url=<span class="string">'xxx'</span>,</span><br><span class="line">    callback=self.parse_detail,</span><br><span class="line">    meta=&#123;</span><br><span class="line">        <span class="string">'item'</span>: Item,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">      item = response.meta[<span class="string">'item'</span>]	<span class="comment">#实际上是self.request.meta的快捷方式</span></span><br><span class="line">     		...</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<h2 id="避免被禁止"><a href="#避免被禁止" class="headerlink" title="避免被禁止"></a>避免被禁止</h2><p>官方给的防止反爬的方法</p>
<ul>
<li><p>循环切换<code>User-Agent</code></p>
</li>
<li><p>禁用cookie(请参见<a href="https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:setting-COOKIES_ENABLED" target="_blank" rel="noopener">COOKIES_ENABLED</a>)，因为有些站点可能使用cookie来发现bot行为。</p>
</li>
<li><p>使用下载延迟(download delays)(2s或更高)。参见<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOAD_DELAY" target="_blank" rel="noopener">DOWNLOAD_DELAY</a>设置。</p>
</li>
<li><p>如果可能的话，使用谷歌缓存<a href="http://www.googleguide.com/cached_pages.html" target="_blank" rel="noopener">Google cache</a>获取页面，而不是直接访问站点</p>
</li>
<li><p>使用IP池循环切换IP。例如，免费的<a href="https://www.torproject.org/" target="_blank" rel="noopener">Tor project</a>或者像<a href="https://proxymesh.com/" target="_blank" rel="noopener">ProxyMesh</a>这样的付费服务。开源的另一种选择是<a href="https://scrapoxy.io/" target="_blank" rel="noopener">scrapoxy</a>，这是一个超级代理，您可以将自己的代理附加到其中。<em>就是设置一个IP池，使用代理，循环使用♻️</em></p>
</li>
<li><p>使用一个高度分布式的下行加载程序，可以在内部绕过禁止，因此您可以只关注解析干净的页面。这样下载的一个例子就是<a href="https://scrapinghub.com/crawlera?_ga=2.29623615.1463224097.1531630415-1657916130.1531387452" target="_blank" rel="noopener">Crawlera</a></p>
</li>
</ul>
<p>……..持续更新</p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/08/04/几种语言中的装饰器/" class="prev">上一篇</a><a href="/2018/07/22/博客破壳/" class="next">下一篇</a></div><div class="copyright"><p>© 2018 <a href="https://blog.ache.fun">阿扯</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/MikeCoder/hexo-theme-gandalfr" target="_blank">hexo-theme-gandalfr</a>.</p></div></footer></div><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js" crossorigin="anonymous"></script><script src="//cdn.bootcss.com/jquery/3.1.1/jquery.js" crossorigin="anonymous"></script><script>$(document).ready(function() { $('pre').each(function(i, block) { hljs.highlightBlock(block); }); });</script></body></html>